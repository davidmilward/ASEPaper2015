 \section{Introduction}

Scientific progress in medicine consists principally in the
introduction of new diagnostics and new therapeutics: new ways of
figuring out what is wrong, and new ways of doing something about it.
The causes of disease, the expression of symptoms, and the response to
treatment, can be extremely complex, involving many levels of
biology---genomics, proteomics, metabolomics---and a wide variety of
lifestyle and environmental factors.

To obtain the evidence to support the introduction of a new diagnostic
or therapeutic, while taking account of the variations in biology and
environment, we need to make detailed, comparable observations of a
suitably large number of individuals.  These observations will be made
by different people, under different circumstances, and stored and
processed in different information systems.  A significant degree of
coordination is required to achieve an adequate combination of detail
and comparability.

One means of achieving this involves prior agreement upon a prescribed
dataset: a precise specification of the observations needed to answer
a particular question.  An information system is then implemented or
configured to match this dataset, and staff are trained to acquire
data to match the prescribed interpretation.  This can be
time-consuming and expensive.  Furthermore, the data acquired is
unlikely to be re-used: not only is the dataset as a whole tailored
towards a particular question, but the same may be true of the
individual observations.

It should be clear that this is unnecessary.  A dataset can be
composed by drawing upon, and adding to, a library of existing data
definitions.  As a precise specification, it can be used as the basis
for the automatic generation or configuration of an information system
for data capture, reducing costs while enhancing the value of the data
obtained.  The enhanced value comes in part from the fact that some of
the data has been recorded against existing definitions, but also from
the fact that definitions are readily available---in a re-usable,
computable form---for all of the data collected.

Costs can be further reduced, and scientific progress accelerated, by
re-using data already collected: in other scientific studies; in the
course of care delivery; or from other sources, including new sensor
technologies.  Precise data definitions are important here also: we
need to know whether the data already collected is fit for purpose,
whether its interpretation is consistent with the analysis that we
intend to perform.  In most cases, these definitions will be created
retrospectively, by progressively adding information about the context
of collection, and any subsequent processing, until the interpretation
of the data is clear.

Having created precise definitions for existing data, we can enhance
the value of the data, and reduce the costs of research, by making
these definitions available in a standardised, computable form.  In
designing and proposing a new clinical study, researchers can take
account not only of what questions have been asked before, but also of
what data already exists, in other research databases or in clinical
information systems.  In this way, we can eliminate unhelpful
variation in study design and unnecessary duplication of effort in
data collection.

Furthermore, if we have both precise definitions of existing data, and
precise definitions of data requirements for a study, then we can
provide automatic support for aspects of study design and delivery:
definitions can be compared, queries can be generated for data
extraction, and---if summary metadata on the contents of existing
databases is available---the feasibility of a particular study can be
assessed in advance.

Scientific progress in medicine is increasingly dependent upon this
kind of automatic support for data management and integration.
Without it, we will be unable to assemble the evidence
needed---detailed, comparable data on thousands or millions of
individuals---to produce new insights, to validate new discoveries,
and to support the introduction of new diagnostics and therapeutics
into clinical practice.

This paper explains how a model-driven approach to software and data
engineering can provide the support required for large-scale clinical
research.  It introduces a simple data modelling language, consistent
with standard object modelling notations, together with a set of tools
for model creation, maintenance, and deployment.  It then reports upon
the experience of applying these tools within two large-scale clinical
research initiatives.

One of these initiatives involves the re-use of data captured in
existing clinical information systems.  In the area of translational
research, in which new innovations are developed and evaluated in the
context of clinical practice (`from bench to bedside'), the data
needed to support the science is often the same as that needed to
support high-quality care delivery and service improvement.  In
existing clinical systems, however, the same observation may be
recorded in many different ways, and there are significant
challenges. 

The other involves the development of new information systems for data
acquisition and management, as well as the customisation and re-use of
existing systems wherever possible.  Here, the emphasis is upon
defining new, generic datasets, in specific therapeutic areas, with
the aim of supporting a wide range of studies using the data
collected.  A particular challenge for informatics development and
data re-use stems from the constant updating of these datasets to take
account of new knowledge, new constraints, and new objectives.

Together, these initiatives provide an initial validation of the
approach.  The software tools are in use by clinical researchers,
rather than the software engineers responsible for their design.  Data
is being collected to the definitions that they have produced, using
software that they have generated.  It is too early to undertake any
quantitative, comparative evaluation; however, it is our hope that a
report of the approach taken, and the experience gained thus far, will
be useful to those working in the field of automated software
engineering.
